{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/training-with-deep-learning/how-to-use-estimator/how-to-use-estimator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Run Using Pytorch Estimator in Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use Azure ML's PyTorch estimator to run our training script locally by using the conda environment created for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"scripts\")\n",
    "sys.path.append(\"scripts/cocoapi/PythonAPI/\")\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "from dotenv import set_key, get_key, find_dotenv\n",
    "from utilities import get_auth, download_data\n",
    "\n",
    "import torch\n",
    "from scripts.XMLDataset import BuildDataset, get_transform\n",
    "from scripts.maskrcnn_model import get_model\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download the dataset that includes the images of store shelves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"Data.zip\"\n",
    "data_url = (\"https://bostondata.blob.core.windows.net/builddata/{}\".format(data_file))\n",
    "download_data(data_file, data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Let's load the existing workspace you created earlier in the Azure ML configuration notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(auth=get_auth(env_path))\n",
    "print(ws.name, ws.resource_group, ws.location, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment and give it a name. The script runs will be recorded under this experiment in Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name='torchvision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a train.py script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scripts/train.py\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A Pytorch Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we pick the number of epochs to run the training for.This deliberately has a low default value for the speed of running. In actual application, set this to higher values (i.e. num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    \"--data_path\": \".\",\n",
    "    \"--workers\": 8,\n",
    "    \"--learning_rate\": 0.005,\n",
    "    \"--epochs\": num_epochs,\n",
    "    \"--anchor_sizes\": \"16,32,64,128,256,512\",\n",
    "    \"--anchor_aspect_ratios\": \"0.25,0.5,1.0,2.0\",\n",
    "    \"--rpn_nms_thresh\": 0.5,\n",
    "    \"--box_nms_thresh\": 0.3,\n",
    "    \"--box_score_thresh\": 0.10,\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_directory=\"./scripts\",\n",
    "    script_params=script_params,\n",
    "    compute_target=\"local\",\n",
    "    entry_script=\"train.py\",\n",
    "    use_docker=False,\n",
    "    user_managed=True,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we point the python interpreter to the local conda environment built for this tutorial. Azure ML SDK will run the training script using this environment. We also turn off project snapshot upload to the cloud since we have a large dataset in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.run_config.environment.python.interpreter_path = (\"/data/anaconda/envs/TorchDetectAML/bin/python\")\n",
    "estimator.run_config.history.snapshot_project = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(estimator)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now register this first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.register_model(model_name=\"torchvision_local_model\", model_path=\"/outputs/model_latest.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download our model and load it to make predictions on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(\"outputs/model_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "anchor_sizes = \"16,32,64,128,256,512\"\n",
    "anchor_aspect_ratios = \"0.25,0.5,1.0,2.0\"\n",
    "rpn_nms_threshold = 0.5\n",
    "box_nms_threshold = 0.3\n",
    "box_score_threshold = 0.1\n",
    "num_box_detections = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mask RCNN model\n",
    "model = get_model(\n",
    "    num_classes,\n",
    "    anchor_sizes,\n",
    "    anchor_aspect_ratios,\n",
    "    rpn_nms_threshold,\n",
    "    box_nms_threshold,\n",
    "    box_score_threshold,\n",
    "    num_box_detections,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model_latest.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random subset of the data to visualize predictions on the images.\n",
    "data_path = \"./scripts\"\n",
    "dataset = BuildDataset(data_path, get_transform(train=False))\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    img, _ = dataset[i]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "    img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "    preds = prediction[0][\"boxes\"].cpu().numpy()\n",
    "    print(prediction[0][\"scores\"])\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for i in range(len(preds)):\n",
    "        draw.rectangle(\n",
    "            ((preds[i][0], preds[i][1]), (preds[i][2], preds[i][3])), outline=\"red\"\n",
    "        )\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we  will [build a custom docker image and push it to Azure Container Registry](03_BuildDockerImage.ipynb). This image will be used for tunning the hyperparameters of the model on AzureMLCompute."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "maxluk"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:TorchDetectAML]",
   "language": "python",
   "name": "conda-env-TorchDetectAML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "msauthor": "minxia"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
