{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.000188,
     "end_time": "2019-07-02T18:00:24.939249",
     "exception": false,
     "start_time": "2019-07-02T18:00:24.939061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Adapted from https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.5e-05,
     "end_time": "2019-07-02T18:00:24.992049",
     "exception": false,
     "start_time": "2019-07-02T18:00:24.992034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare Training Script\n",
    "\n",
    "In this notebook, we create the training script of the Mask R-CNN model that will be tuned. We first define the custom dataset class and the model that finetunes a pre-trained Mask R-CNN for our dataset. The training script is created by appending some notebook cells in turn so it is essential that you run the notebook's cells in order for the script to run correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 8e-06,
     "end_time": "2019-07-02T18:00:24.998298",
     "exception": false,
     "start_time": "2019-07-02T18:00:24.998290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define dataset class and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.014431,
     "end_time": "2019-07-02T18:00:25.018945",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.004514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/XMLDataset.py\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"JPEGImages\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"JPEGImages\", self.imgs[idx])\n",
    "        xml_path = os.path.join(\n",
    "            self.root, \"Annotations\", \"{}.xml\".format(self.imgs[idx].strip(\".jpg\"))\n",
    "        )\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # parse XML annotation\n",
    "        tree = ET.parse(xml_path)\n",
    "        t_root = tree.getroot()\n",
    "\n",
    "        # get bounding box coordinates\n",
    "        boxes = []\n",
    "        for obj in t_root.findall(\"object\"):\n",
    "            bnd_box = obj.find(\"bndbox\")\n",
    "            xmin = float(bnd_box.find(\"xmin\").text)\n",
    "            xmax = float(bnd_box.find(\"xmax\").text)\n",
    "            ymin = float(bnd_box.find(\"ymin\").text)\n",
    "            ymax = float(bnd_box.find(\"ymax\").text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        num_objs = len(boxes)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        # area of the bounding box, used during evaluation with the COCO metric for small, medium and large boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 9e-06,
     "end_time": "2019-07-02T18:00:25.018999",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.018990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.014336,
     "end_time": "2019-07-02T18:00:25.040020",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.025684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/maskrcnn_model.py\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.rpn import RPNHead\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    num_classes,\n",
    "    anchor_sizes,\n",
    "    anchor_aspect_ratios,\n",
    "    rpn_nms_threshold,\n",
    "    box_nms_threshold,\n",
    "    box_score_threshold,\n",
    "    num_box_detections,\n",
    "):\n",
    "\n",
    "    # load pre-trained mask R-CNN model\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "        pretrained=True,\n",
    "        rpn_nms_thresh=rpn_nms_threshold,\n",
    "        box_nms_thresh=box_nms_threshold,\n",
    "        box_score_thresh=box_score_threshold,\n",
    "        box_detections_per_img=num_box_detections,\n",
    "    )\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    anchor_sizes = tuple([float(i) for i in anchor_sizes.split(\",\")])\n",
    "    anchor_aspect_ratios = tuple([float(i) for i in anchor_aspect_ratios.split(\",\")])\n",
    "\n",
    "    # create an anchor_generator for the FPN which by default has 5 outputs\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=tuple([anchor_sizes for _ in range(5)]),\n",
    "        aspect_ratios=tuple([anchor_aspect_ratios for _ in range(5)]),\n",
    "    )\n",
    "    model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "    # get number of input features for the RPN returned by FPN (256)\n",
    "    in_channels = model.backbone.out_channels\n",
    "\n",
    "    # replace the RPN head\n",
    "    model.rpn.head = RPNHead(\n",
    "        in_channels, anchor_generator.num_anchors_per_location()[0]\n",
    "    )\n",
    "\n",
    "    # turn off masks since dataset only has bounding boxes\n",
    "    model.roi_heads.mask_roi_pool = None\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 8e-06,
     "end_time": "2019-07-02T18:00:25.040074",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.040066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define the training script and its arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 8e-06,
     "end_time": "2019-07-02T18:00:25.047306",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.047298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use some of the below arguments as hyperparameters to tune the object detection model later. See following for all [arguments of MaskRCNN](https://github.com/pytorch/vision/blob/7716aba57e6e12a544c42136b274508955526163/torchvision/models/detection/mask_rcnn.py#L20).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.013009,
     "end_time": "2019-07-02T18:00:25.066986",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.053977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/train.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./cocoapi/PythonAPI/\")\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import utils\n",
    "from XMLDataset import BuildDataset, get_transform\n",
    "from maskrcnn_model import get_model\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Object Detection Training\")\n",
    "    parser.add_argument(\n",
    "        \"--data_path\", default=\"./Data/\", help=\"the path to the dataset\"\n",
    "    )\n",
    "    parser.add_argument(\"--batch_size\", default=2, type=int)\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", default=10, type=int, help=\"number of total epochs to run\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--workers\", default=4, type=int, help=\"number of data loading workers\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", default=0.005, type=float, help=\"initial learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"momentum\")\n",
    "    parser.add_argument(\n",
    "        \"--weight_decay\",\n",
    "        default=0.0005,\n",
    "        type=float,\n",
    "        help=\"weight decay (default: 1e-4)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_step_size\", default=3, type=int, help=\"decrease lr every step-size epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_gamma\",\n",
    "        default=0.1,\n",
    "        type=float,\n",
    "        help=\"decrease lr by a factor of lr-gamma\",\n",
    "    )\n",
    "    parser.add_argument(\"--print_freq\", default=10, type=int, help=\"print frequency\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"outputs\", help=\"path where to save\")\n",
    "    parser.add_argument(\"--anchor_sizes\", default=\"16\", type=str, help=\"anchor sizes\")\n",
    "    parser.add_argument(\n",
    "        \"--anchor_aspect_ratios\", default=\"1.0\", type=str, help=\"anchor aspect ratios\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--rpn_nms_thresh\",\n",
    "        default=0.7,\n",
    "        type=float,\n",
    "        help=\"NMS threshold used for postprocessing the RPN proposals\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--box_nms_thresh\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"NMS threshold for the prediction head. Used during inference\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--box_score_thresh\",\n",
    "        default=0.05,\n",
    "        type=float,\n",
    "        help=\"during inference only return proposals\"\n",
    "        \"with a classification score greater than box_score_thresh\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--box_detections_per_img\",\n",
    "        default=100,\n",
    "        type=int,\n",
    "        help=\"maximum number of detections per image, for all classes\",\n",
    "    )\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 9e-06,
     "end_time": "2019-07-02T18:00:25.067040",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.067031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.014362,
     "end_time": "2019-07-02T18:00:25.088301",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.073939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile --append scripts/train.py\n",
    "\n",
    "data_path = args.data_path\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = BuildDataset(data_path, get_transform(train=True))\n",
    "dataset_test = BuildDataset(data_path, get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-100])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-100:])\n",
    "\n",
    "batch_size = args.batch_size\n",
    "workers = args.workers\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=workers,\n",
    "    collate_fn=utils.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 8e-06,
     "end_time": "2019-07-02T18:00:25.088354",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.088346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.010115,
     "end_time": "2019-07-02T18:00:25.105842",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.095727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile --append scripts/train.py\n",
    "\n",
    "# our dataset has two classes only - background and out of stock\n",
    "num_classes = 2\n",
    "\n",
    "model = get_model(\n",
    "    num_classes,\n",
    "    args.anchor_sizes,\n",
    "    args.anchor_aspect_ratios,\n",
    "    args.rpn_nms_thresh,\n",
    "    args.box_nms_thresh,\n",
    "    args.box_score_thresh,\n",
    "    args.box_detections_per_img,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 9e-06,
     "end_time": "2019-07-02T18:00:25.105898",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.105889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.013235,
     "end_time": "2019-07-02T18:00:25.128336",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.115101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile --append scripts/train.py\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = args.learning_rate\n",
    "momentum = args.momentum\n",
    "weight_decay = args.weight_decay\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "lr_step_size = args.lr_step_size\n",
    "lr_gamma = args.lr_gamma\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=lr_step_size, gamma=lr_gamma\n",
    ")\n",
    "\n",
    "# number of training epochs\n",
    "num_epochs = args.epochs\n",
    "print_freq = args.print_freq\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=print_freq)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset after every epoch\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "# save model\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "torch.save(model.state_dict(), os.path.join(args.output_dir, \"model_latest.pth\"))\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 9e-06,
     "end_time": "2019-07-02T18:00:25.128389",
     "exception": false,
     "start_time": "2019-07-02T18:00:25.128380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the next notebook, we [train the model locally and visualize its predictions](02_PytorchEstimatorLocalRun.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TorchDetectAML]",
   "language": "python",
   "name": "conda-env-TorchDetectAML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "papermill": {
   "duration": 2.234311,
   "end_time": "2019-07-02T18:00:25.816698",
   "environment_variables": {},
   "exception": false,
   "output_path": "01_PrepareTrainingScript.ipynb",
   "parameters": {},
   "start_time": "2019-07-02T18:00:23.582387",
   "version": "0.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
